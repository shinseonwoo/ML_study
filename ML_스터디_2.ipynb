{"cells":[{"metadata":{},"cell_type":"markdown","source":"## EDA와 Feature-engineering을 통한 대출 상환여부 예측\n\n#### reference\n* Home Credit Default Risk - A Gentle Introduction <br>\n\nhttps://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction <br>\nhttps://bkshin.tistory.com/entry/캐글-5-Home-Credit-Default-Risk\n"},{"metadata":{},"cell_type":"markdown","source":"### 대회 설명\n각 고객의 정보를 기반으로 해당 고객이 대출한 돈을 갚을 수 있을지 없을지에 대한 확률을 예측하는 대회 <br>\n(supervised classification task)\n> * 0 이면 대출상환 가능, 1이면 대출상환 어려움\n* 주어진 여러 데이터 테이블이 있지만, 이 노트북에서는 메인 테이블 application_train, application_test만 사용할 예정"},{"metadata":{},"cell_type":"markdown","source":"### Metric : ROC AUC\nAUC-ROC 곡선은 다양한 임계값에서 모델의 분류 성능에 대한 측정 그래프\n* ROC = 모든 임계값에서 분류 모델의 성능을 보여주는 그래프\n* AUC(Area Under the Curve) = ROC곡선아래영역\n    * 우수한 분류모델은 AUC값이 1에 가깝고, 클래스를 분류하는 성능이 뛰어남을 의미\n    * AUC 최소값은 0.5으로, 이 경우 모델의 클래스 분리 능력이 전혀 없음을 뜻함\n    \n    > AUC해석\n      * AUC=0.7이면, 해당 분류 모델이 양성 클래스와 음성 클래스를 구별할 수 있는 확률은 70%\n    \n#### ROC-curve에서의 수식\n* True positive rates(=recall, sensitivity)\n    * TPR = R = TP / (TP+FN)\n        * 실제 암 환자 중에서 암환자라고 맞춘 확률\n* True negative rates(=specificity)\n    * TNR = TN / (TN+FP)\n        * 실제 정상인 환자 중에서 정상이라고 맞춘 확률\n* ROC-curve에서 x축, y축\n    * y축 : TPR(=Recall)\n    * x축 : 1-(TNR)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"--------------------------------------------------------\n## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 1) Target 컬럼의 분포를 살펴보자\nTarget은 우리가 예측해야하는 값이다. 0이면 제때 대출금 상환 가능한것, 1이면 상황이 어려운것을 의미한다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 대출을 상환할 수 있는 0값이 1보다 훨씬 많은 imbalanced data이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2) 결측치 확인\n각 컬럼별 결측치 개수 및 비중 확인"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n    # 전체 결측치 개수 확인\n    mis_val=df.isnull().sum()\n    \n    # 결측치 비중 확인\n    mis_val_percent=100*df.isnull().sum()/len(df)\n    \n    # 결측치 개수 , 결측치 비중 테이블 만들기\n    mis_val_table=pd.concat([mis_val, mis_val_percent],axis=1)\n    \n    # 컬럼 이름바꾸기\n    mis_val_table_ren_columns=mis_val_table.rename(columns={0:'Missing Values',1:'% of Total Values'})\n\n    # 결측치 0인 컬럼은 제외하고 정렬\n    mis_val_table_ren_columns=mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of Total Values',ascending=False).round(1)\n\n    # 요약 결과 print\n    print(\"app_train의 전체 컬럼 개수는 \"+str(df.shape[1])+\"개 이다.\\n\"\n         \"그 중에서 결측치가 있는 컬럼 개수는 \"+str(mis_val_table_ren_columns.shape[0])+'개 이다.')\n    \n    return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values=missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* 머신러닝 모델을 만드려면, 위의 결측치들을 채워야한다.(imputation)\n* 뒷부분에서 imputation없이 결측치를 채울 수 있는 XGBoost 모델을 사용할것이다.\n* 또 다른 방법으로는 결측치가 너무 많은 컬럼은 삭제할수도있지만, 해당 컬럼이 모델 성능에 도움이 될수도 있기 때문에 우선 유지하기로한다."},{"metadata":{},"cell_type":"markdown","source":"## 3) Column Types\nint64, float64 타입은 수치형변수이고 object 타입은 범주형 변수"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 범주형 변수에서 유니크한 값의 개수를 살펴보자\n# app_train.select_dtypes('object').apply(pd.Series.nunique)\napp_train.select_dtypes('object').nunique()   # apply함수 없이 가능","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 대부분의 범주형 변수는 유니크한 값이 적은것으로 보인다.\n    * ORGANIZATION_TYPE 와 OCCUPATION_TYPE 는 예외"},{"metadata":{},"cell_type":"markdown","source":"## 4) 범주형 변수 Encoding\nLightGBM같은 모델을 제외하고 대부분의 머신러닝 모델은 범주형 변수를 다룰 수 없기 때문에, 이러한 범주형 변수를 encode해줘야 한다. 그 방법으로는 아래 두가지가 있음\n* 1) Label encoding :\n    * 범주형 변수의 개별값을 숫자로 바꿔주는 방법. 컬럼을 새로 생성하지 않음\n    * 여성/남성 처럼 범주형 변수의 값이 두개일경우는 Label encoding을 사용해도 무관하지만, 그 이상일경우는 One-hot encoding을 사용하는것이 좋음\n* 2) One-hot encoding :\n    * 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방법\n    * One-hot encoding의 경우 범주형 변수의 유니크한값의 개수만큼 컬럼이 늘어난다는것\n        * 이를 보완하기 위해 PCA같은 차원축소 방법을 사용할수도있음\n        \n> 이 노트북에서는 범주형변수의 유니크한 값이 2개일경우 Label encoding을 사용하고 그 이상일 경우 One-hot encoding을 사용할것다."},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding and One-Hot Encoding\n* LabelEncoder(), get_dummies() 활용"},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()\nle_count=0\n\n# 컬럼별로 iterate 돌기\nfor col in app_train:\n    if app_train[col].dtype=='object':\n        # 데이터타입이 object이고 값의 종류가 두개 이하일경우,\n        if len(list(app_train[col].unique())) <=2:\n            \n            # train과 test에 동일하게 라벨인코딩을 하기위해 train기준으로 fit한값을 train,test에 동일하게 transform해줌\n            le.fit(app_train[col])\n            \n            # train-set, test-set 둘다 Transform\n            app_train[col]=le.transform(app_train[col])\n            app_test[col]=le.transform(app_test[col])\n            \n            # 라벨인코딩을 한 컬럼이 몇개인지 카운트\n            le_count+=1\nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 위에서 Label-encoding적용 안한 나머지 범주형 변수에 One-hot encoding 적용\napp_train=pd.get_dummies(app_train)\napp_test=pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5) Train데이터와 Test데이터 컬럼 맞춰주기\ntrain 데이터와 test 데이터에는 동일한 feature가 있어야 한다. <br>\ntrain 데이터에 있는 카테고리변수의 유니크한 값 개수와 test 데이터에 있는 카테고리 변수의 유니크한 값 개수가 다른 변수들이 있어서 one-hot-encoding을 했더니, train에는 있는데 test에 없는 컬럼들이 생겨버림.<br>\n<br>\n따라서 test 데이터에 없고 train에만 있는 컬럼을 삭제해야됨. <br>\n> 우선, train 데이터에서 TARGET 컬럼을 뽑아낸다. \n    * TARGET 컬럼은 test데이터에 없어도 train 데이터에는 반드시 있어야하기 때문에\n\n> align() 함수의 join메소드를 inner로 적용해서 교집합으로 있는 변수만 추린다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Python align() 함수\n* 두 데이터 프레임에 포함 된 데이터를 변경하지 않고 두 데이터 프레임간에 행 및 / 또는 열의 배열이 동일한 지 확인할때 사용"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 예시\ndf1 = pd.DataFrame([[1,2,3,4], [6,7,8,9]], columns=['D', 'B', 'E', 'A'], index=[1,2])\ndf2 = pd.DataFrame([[10,20,30,40], [60,70,80,90], [600,700,800,900]], columns=['A', 'B', 'C', 'D'], index=[2,3,4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 두 데이터프레임에 둘다 포함되어있는 D,B,A 컬럼만 남김\na1, a2 = df1.align(df2, join='inner', axis=1)\nprint(a1)\nprint(a2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TARGET변수는 train데이터에만 있지만 필요한 변수이기때문에 따로 빼두고나서 다시추가할것\ntrain_labels=app_train['TARGET']\n\n\"\"\"\n두 데이터프레임에 모두 있는 컬럼들만 유지하면서 train-set과 test-set을 align한다.\n즉, train 데이터와 test 데이터에 둘다 있는 컬럼들의 값만 가져오려는것\n\"\"\"\n\napp_train, app_test=app_train.align(app_test,join='inner',axis=1)\n\n# TARGET변수 다시 추가\napp_train['TARGET']=train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6) Back to Exploratory Data Analysis\n\n### 6-1) 이상치 (Anomalies)\n* 이상치를 발견할 수 있는 방법중 하나는 describe()메소드로 컬럼의 통계값들을 보는것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# DAYS_BIRTH 컬럼에서는 이상치 없어보임\n(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DAYS_EMPLOYED는 이상치..\napp_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이상치인것 같은 고객들은 따로 빼서 그들의 대출상환 비율이 그외의 고객들에비해 높거나 낮은 경향이 있는지 파악해보자\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이상치\nanom=app_train[app_train['DAYS_EMPLOYED']==365243]\n# 이상치 외\nnon_anom=app_train[app_train['DAYS_EMPLOYED']!=365243]\n\nprint('The non-anomalies default on %0.2f%% of loans' %(100*non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 이상치로 보이는 고객들이 대출을 상환하지못할 확률이 5.4%로 더 낮음.\n* 이상치를 다루는 가장 안전한 방법은 결측치 채우듯이 채우는 방법\n* 이 경우 모든 이상치들이 같은값을 갖고 있으므로, 다 같은 값으로 채울것이다.\n* 이상값들이 중요해보이니, 머신러닝 모델에 이 이상값들을 임의로 채운것에대해 알려줄것이다.\n\n> 결론적으로\n    * 이상값을 숫자로 채우지 않고, 새로운 boolean 컬럼을 만들어서 이상값인지 아닌지를 구분할것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an anomalous flag column\n## 이상치(365243)인 값에 대해서 True , False로 구분\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# 이상치를 nan값으로 대치\napp_train['DAYS_EMPLOYED'].replace({365243:np.nan},inplace=True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test 데이터에도 train 데이터와 동일하게 작업\napp_test['DAYS_EMPLOYED_ANOM']=app_test['DAYS_EMPLOYED']==365243\napp_test['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)\n\n# True, False로 되어있는 데이터 sum하면 True인것 개수 카운팅됨.\nprint('There are %d anomalies in the test data out of %d entries'%(app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6-2) Correlations\n이제 카테고리형 변수와 outlier를 다뤄보자. <br>\n데이터를 이해하는 방법중 하나는 변수간, 그리고 target과의 상관관계를 살펴보는것이다. <br>\n.corr()를 사용해서 변수간, 그리고 target변수와의 Pearson 상관관계를 살펴보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TARGET 변수와의 상관관계\ncorrelations=app_train.corr()['TARGET'].sort_values()\n\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* DAYS_BIRTH 컬럼이 가장 양의 상관성이 높다. 양의 상관을 띄지만, 이 변수의 값들은 실제로 음수이다. \n    * 이 의미는 고객 나이가 많을수록 대출 상환할 가능성이 적다? 라는 해석이 나오는데 DAYS_BIRTH가 음수여서 그렇게 나타난것으로 보임. 따라서, DAYS_BIRTH에 절댓값을 취해서 다시 상관관계를 보려고 함."},{"metadata":{},"cell_type":"markdown","source":"### 6-3) Effect of Age on Repayment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DAYS_BIRTH의 절대값과 TARGET변수와의 상관계수\napp_train['DAYS_BIRTH']=abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 절대값을 취해서 다시 TARGET과의 상관계수를 보니 고객의 나이가 많을수록, 대출을 제때 상환할 가능성이 높다고 나옴."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\n# 고객 나이에 대한 히스토그램 분포 확인\nplt.hist(app_train['DAYS_BIRTH']/365, edgecolor='k',bins=25)\nplt.title('Age of Client');\nplt.xlabel('Age (years)');\nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 위의 분포를 살펴보니 outlier없이 나이가 고르게 분포되어있는 편. \n* 이제 나이가 TARGET에 미치는 영향을 시각화해서 보기위해 KDE plot을 그려볼것이다.\n\n> KDE plot을 사용하는 이유\n* 보통 분포를 확인할 때 히스토그램을 많이 활용한다. 그런데 히스토그램은 구간을 어떻게 설정하냐에 따라 결과물이 매우 달라져서 엉뚱한 결론과 해석을 내릴 수 있음.\n* 그래서 그 대안으로 커널 밀도 추정(KDE) 그래프를 많이 사용함.\n    * 히스토그램 같은 분포를 곡선화해서 나타낸 그래프"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\n\n# 제때 대출을 상환하는 고객의 나이 plot (TARGET=0)\nsns.kdeplot(app_train.loc[app_train['TARGET']==0,'DAYS_BIRTH']/365,label='target==0')\n\n# 제때 대출을 상환하지못하는 고객의 나이 plot (TARGET=1)\nsns.kdeplot(app_train.loc[app_train['TARGET']==1,'DAYS_BIRTH']/365,label='target==1')\n\nplt.xlabel('Age(years)');\nplt.ylabel('Density');\nplt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* target==1(빨간색) 의 분포를 보면 20-30대에 기울어 있는것을 볼 수 있다. 이는 젊은 층일수록 대출 상환을 못할 확률이 높다고 유추할 수 있음.\n* target==0일때와 1일때의 TARGET과의 분포가 상이한것으로 보아 이 변수는 머신러닝 모델에 유용하게 활용될 것으로 보인다.\n* 그럼이제 나이를 나이대 별로 그룹을 나눠서 target=1(대출 상환이 어려운) 의 평균값을 살펴보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 최소 20 최대 70으로해서 총 10개로 그룹핑\n## 결과는 20이상 25미만, 25이상 30미만,,,, 으로 그룹핑됨. 단 (,)는 포함 [,]는 미포함을 의미\nnp.linspace(20,70,num=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ncut() 함수를 사용해서 5살 간격으로 나이대 그룹을 나눠보자. \n그다음, 각 나이대 별로 대출상환을 못하는 비율을 체크\n\"\"\"\n\nage_data=app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH']=age_data['DAYS_BIRTH']/365\n\n# Bin the age data\nage_data['YEARS_BINNED']=pd.cut(age_data['YEARS_BIRTH'],bins=np.linspace(20,70,num=11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation=75);\nplt.xlabel('Age Group (years)');\nplt.ylabel('Failur to Reapy(%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 젊은층일수록 대출을 상환하지 못하는 것으로 나타남\n* 20-25세, 25-30세 30-35세는 각각 약10% 이상 대출을 상환하지 못했고, 55-60세, 60-65세, 65-70세는 5%이하로 대출을 상환하지 못했음."},{"metadata":{},"cell_type":"markdown","source":"### 6-4) Exterior Sources\n* 음의 상관이 가장 높았던 3개의 변수 EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 이다.\n    * 이 변수들은 외부에서 가져온 정규화된 score를 나타낸다.\n* 그럼, 이제 TARGET 변수와 EXT_SOURCE와의 상관관계와 EXT_SOURCE 서로간의 상관관계를 살펴보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_data=app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\next_data_corrs=ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nsns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* EXT_SOURCE와 TARGET 변수는 음의 상관성을 띄므로, EXT_SOURCE값이 증가할수록 대출 상환을 잘한다는 의미로 해석가능.\n* 또한, DAYS_BIRTH 변수는 EXT_SOURCE_1 변수와 양의 상관성이 높은것으로 보아 이 score중 하나는 고객의 나이일것으로 추정된다.\n* 그 다음은 각 EXT_SOURCE 를 TARGET값 별로 나눠서 분포를 살펴보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\nplt.subplot(3,1,1);\nsns.kdeplot(app_train.loc[app_train['TARGET']==0, 'EXT_SOURCE_1'],label='target==0')\nsns.kdeplot(app_train.loc[app_train['TARGET']==1, 'EXT_SOURCE_1'],label='target==1');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n    plt.subplot(3,1,i+1)\n    \n    sns.kdeplot(app_train.loc[app_train['TARGET']==0,source],label='target==0')\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1,source],label='target==1')\n    \n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' %source);\n    plt.ylabel('Density');\nplt.tight_layout(h_pad=2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* EXT_SOURCE_3 변수는 target값에 따라 차이가 가장 큰것으로 보인다.\n* target 과의 상관계수가 그리 높지는 않지만, target이 0인지 1인지에 따라 값이 다른것으로 보아 모델에 영향을 주는 주요 변수라고 판단할 수 있음."},{"metadata":{},"cell_type":"markdown","source":"### 6-5) Pairs Plot\n* EXT_SOURCE 와 DAYS_BIRTH 변수간의 pair plot을 그려보자. \n* pair plot은 각각의 분포를 보여줄 뿐만 아니라, 여러 변수간의 관계도 보여주는 좋은 시각화이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data for plotting\nplot_data=ext_data.drop(columns=['DAYS_BIRTH']).copy()\n\n# 고객 나이 컬럼 추가\nplot_data['YEARS_BIRTH']=age_data['YEARS_BIRTH']\n\n# 결측치 drop\nplot_data=plot_data.dropna().loc[:100000,:]\n\n# 두 컬럼 간의 상관관계를 계산하는 함수 작성\ndef corr_func(x,y,**kwargs):\n    r=np.corrcoef(x,y)[0][1]\n    ax=plt.gca()\n    ax.annotate(\"r={:.2f}\".format(r),\n               xy=(.2, .8),\n               xycoords=ax.transAxes,\n               size=20)\n\n# Create the pairgrid object\n## vars = 변수명 리스트\ngrid=sns.PairGrid(data=plot_data, size=3, diag_sharey=False, hue='TARGET',\n                 vars=plot_data.columns.drop(['TARGET']).tolist())\n\n# 삼각형 위쪽 영역은 산점도\ngrid.map_upper(plt.scatter,alpha=0.2)\n\n# 대각선은 히스토그램\ngrid.map_diag(sns.kdeplot)\n\n# 삼각형 하단은 density plot\ngrid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot',size=32, y=1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 위의 결과에서 빨간색은 대출 상환을 못하는경우, 파란색은 대출 상환하는 경우를 나타냄.\n* EXT_SOURCE_1과 YEARS_BIRTH 간의 양의 선형관계가 나타난다.\n"},{"metadata":{},"cell_type":"markdown","source":"## 7) Feature Engineering\n* 기존 데이터를 활용해서 새로 feature를 추가한다거나, 중요한 변수만 고른다거나, 차원을 줄이는 방식 등 여러가지 feature engineering 방법이 있음.<br>\n이 노트북에서는 아래 두가지 방법의 feature engineering을 해볼것이다.\n\n### 7-1) Polynomial Features\n> 곡선 형태를 띄는 데이터를 제곱, 세제곱의 값으로 만들어서 일차방정식이 되도록 할 수 있음. 이렇게 dataset의 feature를 조정하여 다항식을 일차방정식으로 만들면 Gradient Descent 같은 알고리즘을 사용해서 학습시킬수 있음.\n\n* 여기에서는 EXT_SOURCE_1를 제곱한값과 EXT_SOURCE_2를 제곱한 값, 그리고 EXT_SOURCE_1 x EXT_SOURCE_2 와 EXT_SOURCE_1 x EXT_SOURCE_2^2 같은 두 변수간의 곱을 새로운 변수로 만들 수 있다.이러한 변수를 상호작용항 이라고 한다.\n\n* 어떤 변수 각각은 target변수에 영향을 미치지 않을 수 있지만, 이 두 변수를 결합했을때 target변수에 영향을 미칠 수 있다.\n\n* 상호작용항은 통계모델에서 다수의 변수들의 효과를 파악하기위해 사용되곤한다. 하지만 머신러닝에는 자주 사용되는것을 보지는 못했다. 그래서 한번 이 상호작용항이 모델예측력에 도움이 되는지 체크해볼것이다.\n\n* 아래 코드에서 EXT_SOURCE, DAYS_BIRTH 변수를 사용해서 polynomial feature를 만들어볼것이다.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n우선 다항식 적용할 변수들의 null값을 imputer로 채워준다.\n'''\n# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# 결측치 처리를 위해 imputer 호출\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')\n\n# target값 따로 저장\npoly_target=poly_features['TARGET']\n# target값 제외한 나머지 변수 저장 \npoly_features=poly_features.drop(columns=['TARGET'])\n\n# 결측치 impute로 메꾸기 (train 데이터 기준으로 fit하고, train과 test에 둘다 transform 적용)\n'''\ntrain set 기준의 평균, 중간값 또는 최빈값으로 새로운 데이터의 null값을 채우는것\n'''\npoly_features=imputer.fit_transform(poly_features)\npoly_features_test=imputer.transform(poly_features_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputer 적용 후\npoly_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_features_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomial object with specified degree\npoly_transformer=PolynomialFeatures(degree=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the polynomial features (train데이터 기준으로 fit)\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features=poly_transformer.transform(poly_features)\npoly_features_test=poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* get_feature_names 메소드를 사용해서 다항식 적용한 변수이름 확인"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 35개의 feature가 만들어진것을 확인.이제 이 새로운 Feature들이 target과 상관관계가 있는지 확인해보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n# drop했던 TARGET변수 다시 추가\npoly_features['TARGET']=poly_target\n\n# TARGET변수와의 상관관계 확인\npoly_corrs=poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 다항식으로 만들어진 몇몇 새로운 변수들은 기존 변수보다 상관관계가 더 높다.<br>\n(위에서 기존 변수와 TARGET변수와의 상관관계중 가장 높았던 값이 EXT_SOURCE_3 변수가 -0.18이었는데, 두 변수 EXT_SOURCE_2 EXT_SOURCE_3 를 조합한 변수는 -0.19로 더큼)\n\n* 실제 이 변수가 모델에 영향이 있는지는 이 변수를 넣었을때와 뺐을때 둘다 테스트해보면된다.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# 원본 train 데이터에 새로 만든 다항변수를 merge해서 새로운 데이터셋 만들기\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# 원본 test 데이터에 새로 만든 다항변수를 merge해서 새로운 데이터셋 만들기\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes => train데이터셋 기준으로 align \napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7-2) Domain Knowledge Features\n* CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n* ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n* CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n* DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain=app_train.copy()\napp_test_domain=app_test.copy()\n\n# train데이터에 새로운 변수 추가\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test데이터에 새로운 변수 추가\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* domain기반으로 새로 만든 변수를 TARGET별로 다른 컬러로 KDE plot을 그려보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,20))\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT','ANNUITY_INCOME_PERCENT','CREDIT_TERM','DAYS_EMPLOYED_PERCENT']):\n    plt.subplot(4,1,i+1)\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0,feature],label='target==0')\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1,feature],label='target==1')\n    \n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature);\n    plt.ylabel('Density');\n    \nplt.tight_layout(h_pad=2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* Target=0일때와 1일때 각 변수의 분포가 별 차이가없어서 이 feature가 유의미할지 테스트해보자"},{"metadata":{},"cell_type":"markdown","source":"### Baseline\n- 우리는 대출을 갚지 못할 확률을 예측하고자 한다. 그래서 만약 아예 모르겠다고하면 test set의 모든 관측치에 0.5라고 예측할수도있다. 이렇게하면 AUC ROC값이 0.5로 나올것이다."},{"metadata":{},"cell_type":"markdown","source":"## 8) Logistic Regression Implementation\n- 모든 categorical 변수를 encoding한 것을 사용할것이다. 그리고 결측치를 imputation으로 채울것이고, 변수를 normalizing할것이다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# training 데이터에서 TARGET 변수 drop -> TARGET변수는 결측치처리 및 Scaling대상아니기때문에\nif 'TARGET' in app_train:\n    train=app_train.drop(columns=['TARGET'])\nelse:\n    train=app_train.copy()\n\n# 변수이름\nfeatures=list(train.columns)\n\n# testing 데이터 복사\ntest=app_test.copy()\n\n# 결측치를 median값으로 처리\nimputer = SimpleImputer(strategy='median')\n\n'''\n각 Feature의 값을 일정한 범위 또는 규칙에 따르게 하기 위해서 스케일링을 사용\n'''\n# 각각의 변수를 0~1 사이의 값으로 만들어주는 MinMaxScaler 사용\n## MinMaxScaler 클래스의 인스턴스를 만들어준다\nscaler=MinMaxScaler(feature_range=(0,1))\n\n# training 데이터에 fit\nimputer.fit(train)\n\n# training데이터와 testing데이터에 둘다 transform\n## imputer 처리 하고나면 DataFrame에서 array형태로 바뀜\ntrain=imputer.transform(train)\ntest=imputer.transform(test)\n\n# Scaling\nscaler.fit(train)\ntrain=scaler.transform(train)\ntest=scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 이제 LogisticRegression을 사용해볼것인데, 오버피팅을 조절해주는 regularization 파라미터 C 를 낮춰서 세팅을 해볼것이다. \n- 우선 log_reg라는 이름으로 model을 만들어주고, .fit()을 사용해서 모델을 훈련시키고, .predict_proba()를 사용해서 testing data에 대한 값을 예측할것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg=LogisticRegression(C=0.0001)\n\n# Train on the training data\nlog_reg.fit(train,train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 예측한 결과값은 mx2 배열로 나오는데(m은 관측치 개수),첫번째 컬럼은 target이 0일 확률이고 두번째 컬럼은 target이 1일 확률이다.(따라서, 두 컬럼의 합은 1이되어야함)\n- 우리가 원하는 것은 대출을 갚지 못할 확률이므로, target=1일 확률인 두번째 컬럼을 선택해야한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 두개의 컬럼이 나오는 것을 확인\nlog_reg.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\n# 두번째 컬럼 선택\nlog_reg_pred=log_reg.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission파일의 형식과 동일하게 SK_ID_CURR 와 TARGET이 들어가게 만들어준다\nsubmit=app_test[['SK_ID_CURR']]\nsubmit['TARGET']=log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission 데이터를 csv file로 저장\nsubmit.to_csv('log_reg_baseline.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* LogisticRegression Score : 0.67887"},{"metadata":{},"cell_type":"markdown","source":"## 9) Improved Model: Random Forest\n* 결정트리의 단점을 보완하고 장점은 그대로 가지고 있는 모델. 대표적인 '배깅' 모델이다.(배깅(Bagging)은 bootstrap aggregating의 줄임말)\n* 훈련 과정에서 구성한 다수의 결정 트리들을 랜덤하게 학습시켜 분류 또는 회귀의 결과도출에 사용함.\n* 기본 결정트리는 해당 데이터에 대해 맞춰서 분류를 진행한 것이기 때문에 과적합 현상이 자주 나타나는 단점이 있는데, 랜덤포레스트는 각각의 트리가 독립적으로 학습해서 이런 단점을 개선함.\n\n\n### 하이퍼파라미터 튜닝\n> **n_estimators**\n    * 결정트리의 갯수를 지정 (Default=10)\n    * 무작정 트리 갯수를 늘린다고해서 성능 좋아지는 것 아님.시간이 걸릴 수 있음\n> **random_state**\n    * 랜덤하게 만들어지기 때문에 random_state를 고정해야 같은 결과를 볼 수 있음\n> **verbose**\n    * 실행 과정 출력 여부\n> **n_jobs**\n    * 적합성과 예측성을 위해 병렬로 실행할 작업 수\n    * n_jobs=-1로 지정하면 컴퓨터의 모든 코어를 사용함"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest=RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data에 훈련\nrandom_forest.fit(train, train_labels)\n\n# feature importances 추출\nfeature_importance_values=random_forest.feature_importances_\nfeature_importances=pd.DataFrame({'feature':features, 'importance':feature_importance_values})\n\n# test 데이터에 대해 예측\npredictions=random_forest.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 제출용 dataframe만들기\nsubmit=app_test[['SK_ID_CURR']]\nsubmit['TARGET']=predictions\n\n# csv 파일 저장\nsubmit.to_csv('random_forest_baseline.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* RandomForest Score :"},{"metadata":{},"cell_type":"markdown","source":"### 9-1) Feature engineering한 데이터로 예측해보자\n* (참고) 기존 train 변수에 다항변수 추가한 app_train_poly데이터가 아니라, 다항변수만 있는 poly_features로 예측한값"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = SimpleImputer(strategy = 'median')\n\n# poly_features는 다항변수만 있는 데이터\n# app_train_poly는 기존 train 데이터에 다항변수 추가한 데이터\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data에 훈련시키기\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# test데이터로 예측\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit=app_test[['SK_ID_CURR']]\nsubmit['TARGET']=predictions\n\nsubmit.to_csv('random_forest_baseline_engineered.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Random Forest engineered Score :0.60467"},{"metadata":{},"cell_type":"markdown","source":"### 9-2) Domain기반으로 만든 feature로 예측해보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TARGET변수제거\napp_train_domain=app_train_domain.drop(columns='TARGET')\n# 도메인기반으로 만든 데이터의 변수명 추출\ndomain_features_names=list(app_train_domain.columns)\n\n# 결측치 처리\nimputer=SimpleImputer(strategy='median')\n# imputer처리 해주고나면 DataFrame 형태에서 array형태로 바뀜\ndomain_features=imputer.fit_transform(app_train_domain)\ndomain_features_test=imputer.transform(app_test_domain)\n\n# 랜덤포레스트 모델 만들기\nrandom_forest_domain=RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n\n# 훈련시키기\nrandom_forest_domain.fit(domain_features,train_labels)\n\n# 변수 중요도 추출\nfeature_importance_values_domain=random_forest_domain.feature_importances_\nfeature_importances_domain=pd.DataFrame({'feature':domain_features_names,\n                                        'importance':feature_importance_values_domain})\n\n# test데이터 넣어서 예측하면 TARGET=0일 확률을 예측한 컬럼 한개와 TARGET=1일 확률을 예측한 컬럼 한개가 있는데\n## 여기서 우리가 원하는것은 TARGET=1일때의 확률이므로 두번째 컬럼 선택해서 저장\npredictions=random_forest_domain.predict_proba(domain_features_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit=app_test[['SK_ID_CURR']]\nsubmit['TARGET']=predictions\n\nsubmit.to_csv('random_forest_baseline_domain.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Random Forest domain features :0.68354"},{"metadata":{},"cell_type":"markdown","source":"## 10) Model Interpretation: Feature Importances\n\n* 어떤 변수가 가장 관련이 있는지를 알기위한 가장 간단한 방법은 랜덤포레스트의 feature importances를 확인하는 것이다. EDA과정에서 변수간 상관관계분석을 통해 EXT_SOURCE 변수와 DAYS_BIRTH 변수가 중요한 변수라고 생각해볼수있다.\n* 나중에는 이 feature importances를 사용해서 차원을 줄여볼것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    # 중요도 높은 순으로 나열\n    df=df.sort_values('importance',ascending=False).reset_index()\n    \n    # 중요도 전체합 대비 해당 변수의 중요도 비중변수 추가\n    df['importance_normalized']=df['importance']/df['importance'].sum()\n    \n    # 시각화\n    plt.figure(figsize=(10,6))\n    ax=plt.subplot()\n    \n    ax.barh(list(reversed(list(df.index[:15]))), df['importance_normalized'].head(15), align='center',edgecolor='k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature engineering안한 기본 변수들로 변수 중요도 추출\nfeature_importances_sorted=plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 예상했던것 처럼 EXT_SOURCE 변수와 DAYS_BIRTH 변수가 중요변수로 나온것을 확인.\n* feature importances가 모델을 해석하고 차원을 줄이는데 가장 좋은 방법이라고 할 수는 없지만, 예측을 할때 모델이 어떤 요인을 고려하는지를 이해하는데 도움이 된다."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# domain기반으로 만든 변수 대상으로 변수 중요도 도출\nfeature_importances_domain_sorted=plot_feature_importances(feature_importances_domain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 도메인 기반으로 만든 4개의 변수가 변수 중요도 top15에 포함되어있는 것을 볼 수 있다. \n> 결론적으로, 도메인기반으로 만든 변수를 모델에 포함했을때가 score가 가장 좋았음."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}